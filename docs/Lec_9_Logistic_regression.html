<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="copyright" content="Presentation link: https://goo.gl/8pffGO"/>
  <title>Logistic regression</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link href="site_libs/slidy-2/styles/slidy.css" rel="stylesheet" />
  <script src="site_libs/slidy-2/scripts/slidy.js"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Logistic regression</h1>
  <p class="author">

  </p>
  <p class="date">20.04.2017</p>
</div>
<div id="what-we-achieved" class="slide section level3">
<h1>1. What have we achieved?</h1>
<p>Variable types</p>
<ul>
<li>numeric</li>
<li>categorical</li>
</ul>
<p>What we have achieved:</p>
<ul>
<li>Confidence intervals (numeric variable)</li>
<li>t-tests, Mann–Whitney (means of numeric variable by one categorical variable with two levels)</li>
<li>χ², Fisher test (categorical by categorical)</li>
<li>Simple linear regression (numeric by one numeric variable)</li>
<li>ANOVA (means of numeric variable by categorical any variable)</li>
<li>Multiple linear regression (numeric by several numeric variables)</li>
<li>Multiple linear regression with dummy variables (numeric by any variable)</li>
</ul>
<p>Today:</p>
<ul>
<li>Logistic (logit) regression (binary dependent variable by any number variables of any type)</li>
</ul>
</div>
<div id="how-it-works" class="slide section level3">
<h1>2.1 How it works</h1>
<p>Logistic or logit regression was developed in [Cox 1958]. It is a regression model wich predicts binary dependent variable using any number of variables of any type.</p>
<p>What do we need?</p>
<p><span class="math display">\[\underbrace{y_i}_{[-\infty, +\infty]}=\underbrace{\mbox{β}_0+\mbox{β}_1\cdot x_1+\mbox{β}_2\cdot x_2 + \dots +\mbox{β}_k\cdot x_k +\mbox{ε}_i}_{[-\infty, +\infty]}\]</span></p>
<p>But in our case <span class="math inline">\(y\)</span> is a binary variable.</p>
<ul>
<li>Probability?</li>
</ul>
<p><span class="math display">\[P(y) = \frac{\mbox{# successes}}{\mbox{# failures} + \mbox{# successes}}; P(y) \in [0, 1]\]</span></p>
<ul>
<li>Odds?</li>
</ul>
<p><span class="math display">\[odds(y) = \frac{P(y)}{1-P(y)} = \frac{\mbox{P(successes)}}{\mbox{P(failures)}} = \frac{\mbox{# successes}}{\mbox{# failures}}; odds(y) \in [0, +\infty]\]</span></p>
<ul>
<li>Natural logarithm of odds</li>
</ul>
<p><span class="math display">\[\log(odds(y)) \in [-\infty, +\infty]\]</span></p>
</div>
<div id="reminder-about-logarithms" class="slide section level3">
<h1>2.2 Reminder about logarithms</h1>
<p><img src="Lec_9_Logistic_regression_files/figure-slidy/unnamed-chunk-2-1.png" width="768" /></p>
<ul>
<li>if log(odds) are greater then 0, it means that we have more successes then failures;</li>
<li>if log(odds) is equal to 0, it means that we have the same number of successes and failures;</li>
<li>if log(odds) are less then 0, it means that we have less successes then failures;</li>
</ul>
</div>
<div id="probability-logodds" class="slide section level3">
<h1>2.3 probability ←→ log(odds)</h1>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="right">s</th>
<th align="right">f</th>
<th align="right">P(s)</th>
<th align="right">odds(s)</th>
<th align="right">log(odds(s))</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">10</td>
<td align="right">0.0</td>
<td align="right">0.0000000</td>
<td align="right">-Inf</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">9</td>
<td align="right">0.1</td>
<td align="right">0.1111111</td>
<td align="right">-2.1972246</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">8</td>
<td align="right">0.2</td>
<td align="right">0.2500000</td>
<td align="right">-1.3862944</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">7</td>
<td align="right">0.3</td>
<td align="right">0.4285714</td>
<td align="right">-0.8472979</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">6</td>
<td align="right">0.4</td>
<td align="right">0.6666667</td>
<td align="right">-0.4054651</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">5</td>
<td align="right">0.5</td>
<td align="right">1.0000000</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">4</td>
<td align="right">0.6</td>
<td align="right">1.5000000</td>
<td align="right">0.4054651</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">3</td>
<td align="right">0.7</td>
<td align="right">2.3333333</td>
<td align="right">0.8472979</td>
</tr>
<tr class="odd">
<td align="right">8</td>
<td align="right">2</td>
<td align="right">0.8</td>
<td align="right">4.0000000</td>
<td align="right">1.3862944</td>
</tr>
<tr class="even">
<td align="right">9</td>
<td align="right">1</td>
<td align="right">0.9</td>
<td align="right">9.0000000</td>
<td align="right">2.1972246</td>
</tr>
<tr class="odd">
<td align="right">10</td>
<td align="right">0</td>
<td align="right">1.0</td>
<td align="right">Inf</td>
<td align="right">Inf</td>
</tr>
</tbody>
</table>
</div>
<p><span class="math display">\[\log(odds(s)) = \log\left(\frac{\#s}{\#f}\right)\]</span> <span class="math display">\[P(s) = \frac{\exp(\log(odds(s)))}{1+\exp(\log(odds(s)))}\]</span></p>
<p>Results of the logistic regression can be easily converted to probabilities.</p>
</div>
<div id="sigmoid" class="slide section level3">
<h1>2.4 Sigmoid</h1>
<p><img src="Lec_9_Logistic_regression_files/figure-slidy/unnamed-chunk-4-1.png" width="768" /></p>
<p>Formula for this sigmoid is the following:</p>
<p><span class="math display">\[y = \frac{1}{1+e^{-x}}\]</span></p>
<p>Feeting our logistic regression we should be able to reverse our sigmoid:</p>
<p><img src="Lec_9_Logistic_regression_files/figure-slidy/unnamed-chunk-5-1.png" width="768" /></p>
<p>Formula for this sigmoid is the following:</p>
<p><span class="math display">\[y = \frac{1}{1+e^{-(-x)}} = \frac{1}{1+e^{x}}\]</span></p>
<p>Feeting our logistic regression we should be able to move center of our sigmoid to the left/right side:</p>
<p><img src="Lec_9_Logistic_regression_files/figure-slidy/unnamed-chunk-6-1.png" width="768" /></p>
<p>Formula for this sigmoid is the following:</p>
<p><span class="math display">\[y = \frac{1}{1+e^{-(x-2)}}\]</span></p>
<p>Feeting our logistic regression we should be able to squeeze/stretch center of our sigmoid:</p>
<p><img src="Lec_9_Logistic_regression_files/figure-slidy/unnamed-chunk-7-1.png" width="768" /></p>
<p><span class="math display">\[y = \frac{1}{1+e^{-4x}}\]</span></p>
<p>So the more general formula will be: <span class="math display">\[y = \frac{1}{1+e^{-k(x-z)}}\]</span></p>
<p>where</p>
<ul>
<li>depending on <span class="math inline">\(x\)</span> values sigmoid can be reversed</li>
<li><span class="math inline">\(k\)</span> is squeeze/stretch coefficient</li>
<li><span class="math inline">\(z\)</span> is coefficient that indicates movement of the sigmoid center to the left or right side</li>
</ul>
</div>
<div id="numeric-example" class="slide section level3">
<h1>3.1 Numeric example</h1>
<p>It was interesting, whether languages with ejective sounds have in average more consonants. So I collected data from phonological database LAPSyD: <a href="http://goo.gl/0btfKa" class="uri">http://goo.gl/0btfKa</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ej_cons &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;http://goo.gl/0btfKa&quot;</span>)
ej_cons %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(ejectives, n.cons.lapsyd, <span class="dt">color =</span> ejectives))+
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">width =</span> <span class="fl">0.2</span>)+
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Number of consonants ~ presence of ejectives&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;presence of ejectives&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;number of consonants&quot;</span>)+
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="Lec_9_Logistic_regression_files/figure-slidy/unnamed-chunk-8-1.png" width="768" /></p>
<ul>
<li>Model without predictors</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit1 &lt;-<span class="st"> </span><span class="kw">glm</span>(ejectives~<span class="dv">1</span>, <span class="dt">data =</span> ej_cons, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)
<span class="kw">summary</span>(fit1)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = ejectives ~ 1, family = &quot;binomial&quot;, data = ej_cons)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9619  -0.9619  -0.9619   1.4094   1.4094  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)  -0.5306     0.3985  -1.331    0.183
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 35.594  on 26  degrees of freedom
## Residual deviance: 35.594  on 26  degrees of freedom
## AIC: 37.594
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>How we get this estimate value?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(ej_cons$ejectives)</code></pre></div>
<pre><code>## 
##  no yes 
##  17  10</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">log</span>(<span class="dv">10</span>/<span class="dv">17</span>)</code></pre></div>
<pre><code>## [1] -0.5306283</code></pre>
<p>What does this model say? This model says that if we have no predictors and take some language it has <span class="math inline">\(\frac{0.5306283}{(1+e^{-0.5306283})} = 0.3340993\)</span> probability to have ejectives.</p>
<ul>
<li>Model with numeric predictor</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit2 &lt;-<span class="st"> </span><span class="kw">glm</span>(ejectives~n.cons.lapsyd, <span class="dt">data =</span> ej_cons, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)
<span class="kw">summary</span>(fit2)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = ejectives ~ n.cons.lapsyd, family = &quot;binomial&quot;, 
##     data = ej_cons)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.8317  -0.4742  -0.2481   0.1914   2.1997  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)    -9.9204     3.7699  -2.631   0.0085 **
## n.cons.lapsyd   0.3797     0.1495   2.540   0.0111 * 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 35.594  on 26  degrees of freedom
## Residual deviance: 16.202  on 25  degrees of freedom
## AIC: 20.202
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>What does this model say? This model says:</p>
<p><span class="math display">\[\log(odds(ej)) = \beta_o + \beta_1 \times n.cons.lapsyd = 
-9.9204 + 0.3797 \times n.cons.lapsyd\]</span></p>
<p>Lets visualize our model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ej_cons %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="st">`</span><span class="dt">P(ejective)</span><span class="st">`</span> =<span class="st"> </span><span class="kw">as.numeric</span>(ejectives) -<span class="st"> </span><span class="dv">1</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> n.cons.lapsyd, <span class="dt">y =</span> <span class="st">`</span><span class="dt">P(ejective)</span><span class="st">`</span>))+
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="dt">method.args =</span> <span class="kw">list</span>(<span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">geom_point</span>()+
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="Lec_9_Logistic_regression_files/figure-slidy/unnamed-chunk-12-1.png" width="768" /></p>
<p>So probability for a language that have 30 consonants will be <span class="math display">\[\log(odds(ej)) = -9.9204 + 0.3797 \times 30 = 1.4706\]</span></p>
<p><span class="math display">\[P(ej) = \frac{1.47061}{1+1.4706}=0.8131486\]</span></p>
</div>
<div id="predict" class="slide section level3">
<h1>4. predict()</h1>
<p>Do we really need to remember all this formulae?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">n.cons.lapsyd =</span> <span class="kw">c</span>(<span class="dv">30</span>, <span class="dv">55</span>, <span class="dv">34</span>, <span class="dv">10</span>))
<span class="kw">predict</span>(fit2, new.df) <span class="co"># odds</span></code></pre></div>
<pre><code>##         1         2         3         4 
##  1.470850 10.963579  2.989686 -6.123334</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(fit2, new.df, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="co"># probabilities</span></code></pre></div>
<pre><code>##           1           2           3           4 
## 0.813186486 0.999982679 0.952106011 0.002186347</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(fit2, new.df, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>, <span class="dt">se.fit =</span> <span class="ot">TRUE</span>) <span class="co"># probabilities and confidense interval</span></code></pre></div>
<pre><code>## $fit
##           1           2           3           4 
## 0.813186486 0.999982679 0.952106011 0.002186347 
## 
## $se.fit
##            1            2            3            4 
## 1.512886e-01 7.882842e-05 6.869366e-02 5.038557e-03 
## 
## $residual.scale
## [1] 1</code></pre>
<p>So we actually can create a plot with confidense intervals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ej_cons_ci &lt;-<span class="st"> </span><span class="kw">cbind.data.frame</span>(ej_cons, <span class="kw">predict</span>(fit2, ej_cons, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>, <span class="dt">se.fit =</span> <span class="ot">TRUE</span>)[<span class="dv">1</span>:<span class="dv">2</span>])
ej_cons_ci</code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="left">name</th>
<th align="right">n.cons.lapsyd</th>
<th align="left">ejectives</th>
<th align="right">fit</th>
<th align="right">se.fit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Turkish</td>
<td align="right">24</td>
<td align="left">no</td>
<td align="right">0.3084436</td>
<td align="right">0.1376976</td>
</tr>
<tr class="even">
<td align="left">Korean</td>
<td align="right">21</td>
<td align="left">no</td>
<td align="right">0.1249319</td>
<td align="right">0.0935836</td>
</tr>
<tr class="odd">
<td align="left">Tiwi</td>
<td align="right">21</td>
<td align="left">no</td>
<td align="right">0.1249319</td>
<td align="right">0.0935836</td>
</tr>
<tr class="even">
<td align="left">Kpelle</td>
<td align="right">22</td>
<td align="left">no</td>
<td align="right">0.1726696</td>
<td align="right">0.1090491</td>
</tr>
<tr class="odd">
<td align="left">Tulu</td>
<td align="right">21</td>
<td align="left">no</td>
<td align="right">0.1249319</td>
<td align="right">0.0935836</td>
</tr>
<tr class="even">
<td align="left">Mapudungun</td>
<td align="right">20</td>
<td align="left">no</td>
<td align="right">0.0889728</td>
<td align="right">0.0780648</td>
</tr>
<tr class="odd">
<td align="left">Kiowa</td>
<td align="right">19</td>
<td align="left">no</td>
<td align="right">0.0626231</td>
<td align="right">0.0634145</td>
</tr>
<tr class="even">
<td align="left">Guarani</td>
<td align="right">18</td>
<td align="left">no</td>
<td align="right">0.0437026</td>
<td align="right">0.0503461</td>
</tr>
<tr class="odd">
<td align="left">Japanese</td>
<td align="right">15</td>
<td align="left">no</td>
<td align="right">0.0144175</td>
<td align="right">0.0227888</td>
</tr>
<tr class="even">
<td align="left">Batak</td>
<td align="right">17</td>
<td align="left">no</td>
<td align="right">0.0303138</td>
<td align="right">0.0392188</td>
</tr>
<tr class="odd">
<td align="left">Yoruba</td>
<td align="right">18</td>
<td align="left">no</td>
<td align="right">0.0437026</td>
<td align="right">0.0503461</td>
</tr>
<tr class="even">
<td align="left">Finnish</td>
<td align="right">17</td>
<td align="left">no</td>
<td align="right">0.0303138</td>
<td align="right">0.0392188</td>
</tr>
<tr class="odd">
<td align="left">Kayardild</td>
<td align="right">17</td>
<td align="left">no</td>
<td align="right">0.0303138</td>
<td align="right">0.0392188</td>
</tr>
<tr class="even">
<td align="left">Hawaiian</td>
<td align="right">8</td>
<td align="left">no</td>
<td align="right">0.0010243</td>
<td align="right">0.0026588</td>
</tr>
<tr class="odd">
<td align="left">Maori</td>
<td align="right">10</td>
<td align="left">no</td>
<td align="right">0.0021863</td>
<td align="right">0.0050386</td>
</tr>
<tr class="even">
<td align="left">Hungarian</td>
<td align="right">26</td>
<td align="left">no</td>
<td align="right">0.4880055</td>
<td align="right">0.1637595</td>
</tr>
<tr class="odd">
<td align="left">Kannada</td>
<td align="right">30</td>
<td align="left">no</td>
<td align="right">0.8131865</td>
<td align="right">0.1512886</td>
</tr>
<tr class="even">
<td align="left">Georgean</td>
<td align="right">28</td>
<td align="left">yes</td>
<td align="right">0.6707173</td>
<td align="right">0.1740778</td>
</tr>
<tr class="odd">
<td align="left">Ingush</td>
<td align="right">34</td>
<td align="left">yes</td>
<td align="right">0.9521060</td>
<td align="right">0.0686937</td>
</tr>
<tr class="even">
<td align="left">Abkhaz</td>
<td align="right">58</td>
<td align="left">yes</td>
<td align="right">0.9999945</td>
<td align="right">0.0000277</td>
</tr>
<tr class="odd">
<td align="left">Amharic</td>
<td align="right">32</td>
<td align="left">yes</td>
<td align="right">0.9029348</td>
<td align="right">0.1088031</td>
</tr>
<tr class="even">
<td align="left">Sandawe</td>
<td align="right">47</td>
<td align="left">yes</td>
<td align="right">0.9996389</td>
<td align="right">0.0012168</td>
</tr>
<tr class="odd">
<td align="left">Tlingit</td>
<td align="right">42</td>
<td align="left">yes</td>
<td align="right">0.9975940</td>
<td align="right">0.0063371</td>
</tr>
<tr class="even">
<td align="left">Lakota</td>
<td align="right">30</td>
<td align="left">yes</td>
<td align="right">0.8131865</td>
<td align="right">0.1512886</td>
</tr>
<tr class="odd">
<td align="left">Yucatec</td>
<td align="right">20</td>
<td align="left">yes</td>
<td align="right">0.0889728</td>
<td align="right">0.0780648</td>
</tr>
<tr class="even">
<td align="left">Aymara</td>
<td align="right">27</td>
<td align="left">yes</td>
<td align="right">0.5821783</td>
<td align="right">0.1725265</td>
</tr>
<tr class="odd">
<td align="left">Pomo</td>
<td align="right">26</td>
<td align="left">yes</td>
<td align="right">0.4880055</td>
<td align="right">0.1637595</td>
</tr>
</tbody>
</table>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ej_cons_ci %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="st">`</span><span class="dt">P(ejective)</span><span class="st">`</span> =<span class="st"> </span><span class="kw">as.numeric</span>(ejectives) -<span class="st"> </span><span class="dv">1</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> n.cons.lapsyd, <span class="dt">y =</span> <span class="st">`</span><span class="dt">P(ejective)</span><span class="st">`</span>))+
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="dt">method.args =</span> <span class="kw">list</span>(<span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>)+
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_pointrange</span>(<span class="kw">aes</span>(<span class="dt">x =</span> n.cons.lapsyd, <span class="dt">ymin =</span> fit -<span class="st"> </span>se.fit, <span class="dt">ymax =</span> fit +<span class="st"> </span>se.fit))+
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;P(ej) ~ number of consonants&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;number of consonants&quot;</span>,
       <span class="dt">caption =</span> <span class="st">&quot;data from LAPSyD database&quot;</span>)+
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="Lec_9_Logistic_regression_files/figure-slidy/unnamed-chunk-15-1.png" width="768" /></p>
</div>

  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

</body>
</html>
